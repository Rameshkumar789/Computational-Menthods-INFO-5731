{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Korlakaunta_In_class_exercise_06.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rameshkumar789/Computational-Menthods-INFO-5731/blob/main/Korlakaunta_In_class_exercise_06.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7TahL04sVvR"
      },
      "source": [
        "# **The sixth in-class-exercise (20 points in total, 3/2/2021)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejyZITr8sjnh"
      },
      "source": [
        "## **1. Rule-based information extraction (10 points)**\n",
        "\n",
        "Use any keywords related to data science, natural language processing, machine learning to search from google scholar, get the **titles** of 100 articles (either by web scraping or manually) about this topic, define a set of patterns to extract the research questions/problems, methods/algorithms/models, datasets, applications, or any other important information about this topic. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvR_O9D8sOUY"
      },
      "source": [
        "import pandas as pd\n",
        "from google.colab import files"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "bFUIy_Gg0FGf",
        "outputId": "8c9d6de1-92eb-430f-89b4-e18d52347f74"
      },
      "source": [
        "uploaded = files.upload()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3dfe115c-6887-4732-aa94-0f2ba61a0255\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-3dfe115c-6887-4732-aa94-0f2ba61a0255\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving articles.csv to articles.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64DWde2Bzz-m"
      },
      "source": [
        "data=pd.read_csv('/content/articles.csv')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZB3UJSozz-m"
      },
      "source": [
        "articles='. '.join(data.Titles)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "HrDfpzADzz-n",
        "outputId": "b9f5b23e-75b2-4a63-dea4-1880a42407db"
      },
      "source": [
        "articles"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"A survey of text summarization techniques. An effective sentence-extraction technique using contextual information and statistical approaches for text summarization. Automatic text summarization: Past, present and future. Graph-based text summarization using modified TextRank. Sentence similarity estimation for text summarization using deep learning. Text summarization: a brief review. Automatic text summarization within big data frameworks. Gather customer concerns from online product reviews–A text summarization approach. Abstract text summarization with a convolutional Seq2seq model. Assessing sentence scoring techniques for extractive text summarization. The use of domain-specific concepts in biomedical text summarization. Text summarization using a trainable summarizer and latent semantic analysis. Text Summarization with Automatic Keyword Extraction in Telugu e-Newspapers. GA, MR, FFNN, PNN and GMM based models for automatic text summarization. Collaborative ranking-based text summarization using a metaheuristic approach. A new graph-based algorithm for Persian text summarization. Text summarization for big data: A comprehensive survey. Word-sentence co-ranking for automatic extractive text summarization. Responsive text summarization. A new automatic multi-document text summarization using topic modeling. Extractive text summarization system to aid data extraction from full text in systematic review development. MCMR: Maximum coverage and minimum redundant text summarization model. PSO-Based Text Summarization Approach Using Sentiment Analysis. A multi-document summarization system based on statistics and linguistic treatment. Supporting main idea identification and text summarization in middle school co-taught classes. COMPENDIUM: A text summarization system for generating abstracts of research papers. An improvised extractive approach to hindi text summarization. A new sentence similarity measure and sentence based extractive technique for automatic text summarization. Text summarization using unsupervised deep learning. Text summarization in the biomedical domain: a systematic review of recent research. Abstractive text summarization using LSTM-CNN based deep learning. Karcı summarization: A simple and effective approach for automatic text summarization using Karcı entropy. Deep learning in the domain of multi-document text summarization. Selection criteria for text mining approaches. A modification to graph based approach for extraction based automatic text summarization. A text summarization method based on fuzzy rules and applicable to automated assessment. Automatic Text Summarization Using Fuzzy Extraction. A complex network approach to text summarization. Text summarization based on classification using ANFIS. Text summarization from legal documents: a survey. Text document summarization using word embedding. Graph based technique for hindi text summarization. Extraction based text summarization methods on user's review data: A comparative study. Opinion mining from online hotel reviews–a text summarization approach. The effectiveness of automatic text summarization in mobile learning contexts. Recent automatic text summarization techniques: a survey. Deep contextualized embeddings for quantifying the informative content in biomedical text summarization. How to improve text summarization and classification by mutual cooperation on an integrated framework. Figure-associated text summarization and evaluation. Extractive multi-document text summarization using a multi-objective artificial bee colony optimization approach. CRHASum: extractive text summarization with contextualized-representation hierarchical-attention summarization network. Literature study on multi-document text summarization techniques. A proposed methodology for subjective evaluation of video and text summarization. A Bengali text generation approach in context of abstractive text summarization using rnn. A two-stage Chinese text summarization algorithm using keyword information and adversarial learning. A comprehensive survey on extractive and abstractive techniques for text summarization. ATSSC: Development of an approach based on soft computing for text summarization. Text Summarization: An Extractive Approach. A variable dimension optimization approach for text summarization. Multidocument arabic text summarization based on clustering and Word2Vec to reduce redundancy. Extractive summarization of a document using lexical chains. Comparison of PubMed, Scopus, web of science, and Google scholar: strengths and weaknesses. Fuzzy swarm diversity hybrid model for text summarization. Speech-to-text summarization using automatic phrase extraction from recognized text. Rulingbr: A summarization dataset for legal texts. Application of Extractive Text Summarization Algorithms to Speech-to-Text Media. Text summarization using Wikipedia. SRL-ESA-TextSum: A text summarization approach based on semantic role labeling and explicit semantic analysis. A multilingual study of compressive cross-language text summarization. A Study of Text Summarization Techniques for Generating Meeting Minutes. Minimum redundancy and maximum relevance for single and multi-document Arabic text summarization. Extractive summarization of clinical trial descriptions. Using query expansion in graph-based approach for query-focused multi-document summarization. Extractive arabic text summarization using modified PageRank algorithm. Evaluating different similarity measures for automatic biomedical text summarization. Hybrid text summarization: a survey. Semantic Graph Based Automatic Text Summarization for Hindi Documents Using Particle Swarm Optimization. Towards automatic tweet generation: A comparative study from the text summarization perspective in the journalism genre. Abstractive multi-document text summarization using a genetic algorithm. Long-span language models for query-focused unsupervised extractive text summarization. An improved method of automatic text summarization for web contents using lexical chain with semantic-related terms. Cross-language text summarization using sentence and multi-sentence compression. Extractive multi-document text summarization based on graph independent sets. Cross-lingual speech-to-text summarization. Abstractive social media text summarization using selective reinforced Seq2Seq attention model. A hybrid approach for arabic text summarization using domain knowledge and genetic algorithms. Abstractive text summarization based on improved semantic graph approach. Comparative study of DE and PSO over document summarization. Facilitating physicians' access to information via tailored text summarization. A proposed natural language processing preprocessing procedures for enhancing arabic text summarization. Different approaches for identifying important concepts in probabilistic biomedical text summarization. Extractive Text Summarization Models for Urdu Language. Analysis of competitor intelligence in the era of big data: an integrated system using text summarization based on global optimization. Text summarization using wordnet graph based sentence ranking. Corpus-based problem selection for EHR note summarization. Summarization of legal judgments using gravitational search algorithm. Text summarization. Hybrid latent semantic analysis and random indexing model for text summarization. Sumdoc: a unified approach for automatic text summarization. Improving text summarization of online hotel reviews with review helpfulness and sentiment\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0pykvhh0ctA"
      },
      "source": [
        "### Rule-based Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMp4teHlzz-n"
      },
      "source": [
        "import nltk \n",
        "import spacy \n",
        "import pandas as pd \n",
        "import numpy as np \n",
        "import math \n",
        "from tqdm import tqdm\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6aqe9cCzz-o"
      },
      "source": [
        "from spacy.matcher import Matcher \n",
        "from spacy.tokens import Span "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUxMzGHyzz-o"
      },
      "source": [
        "# load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(articles)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzNw_Cihzz-o",
        "outputId": "748639c6-73d3-4ee7-a317-fc1869cbf40b"
      },
      "source": [
        "for tok in doc:\n",
        "    print(tok.text, \"-->\",tok.dep_,\"-->\", tok.pos_)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A --> det --> DET\n",
            "survey --> ROOT --> NOUN\n",
            "of --> prep --> ADP\n",
            "text --> compound --> NOUN\n",
            "summarization --> compound --> NOUN\n",
            "techniques --> pobj --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "An --> det --> DET\n",
            "effective --> amod --> ADJ\n",
            "sentence --> compound --> NOUN\n",
            "- --> punct --> PUNCT\n",
            "extraction --> compound --> NOUN\n",
            "technique --> ROOT --> NOUN\n",
            "using --> acl --> VERB\n",
            "contextual --> amod --> ADJ\n",
            "information --> dobj --> NOUN\n",
            "and --> cc --> CCONJ\n",
            "statistical --> amod --> ADJ\n",
            "approaches --> conj --> NOUN\n",
            "for --> prep --> ADP\n",
            "text --> compound --> NOUN\n",
            "summarization --> pobj --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "Automatic --> amod --> ADJ\n",
            "text --> compound --> NOUN\n",
            "summarization --> ROOT --> NOUN\n",
            ": --> punct --> PUNCT\n",
            "Past --> ROOT --> ADJ\n",
            ", --> punct --> PUNCT\n",
            "present --> conj --> ADJ\n",
            "and --> cc --> CCONJ\n",
            "future --> conj --> ADJ\n",
            ". --> punct --> PUNCT\n",
            "Graph --> npadvmod --> PROPN\n",
            "- --> punct --> PUNCT\n",
            "based --> amod --> VERB\n",
            "text --> compound --> NOUN\n",
            "summarization --> ROOT --> NOUN\n",
            "using --> acl --> VERB\n",
            "modified --> amod --> VERB\n",
            "TextRank --> dobj --> PROPN\n",
            ". --> punct --> PUNCT\n",
            "Sentence --> compound --> NOUN\n",
            "similarity --> compound --> NOUN\n",
            "estimation --> ROOT --> NOUN\n",
            "for --> prep --> ADP\n",
            "text --> compound --> NOUN\n",
            "summarization --> pobj --> NOUN\n",
            "using --> acl --> VERB\n",
            "deep --> amod --> ADJ\n",
            "learning --> dobj --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "Text --> compound --> NOUN\n",
            "summarization --> ROOT --> NOUN\n",
            ": --> punct --> PUNCT\n",
            "a --> det --> DET\n",
            "brief --> amod --> ADJ\n",
            "review --> appos --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "Automatic --> amod --> ADJ\n",
            "text --> compound --> NOUN\n",
            "summarization --> ROOT --> NOUN\n",
            "within --> prep --> ADP\n",
            "big --> amod --> ADJ\n",
            "data --> compound --> NOUN\n",
            "frameworks --> pobj --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "Gather --> amod --> VERB\n",
            "customer --> compound --> NOUN\n",
            "concerns --> ROOT --> NOUN\n",
            "from --> prep --> ADP\n",
            "online --> amod --> ADJ\n",
            "product --> compound --> NOUN\n",
            "reviews --> pobj --> NOUN\n",
            "– --> punct --> PUNCT\n",
            "A --> det --> DET\n",
            "text --> compound --> NOUN\n",
            "summarization --> compound --> NOUN\n",
            "approach --> ROOT --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "Abstract --> amod --> ADJ\n",
            "text --> compound --> NOUN\n",
            "summarization --> ROOT --> NOUN\n",
            "with --> prep --> ADP\n",
            "a --> det --> DET\n",
            "convolutional --> amod --> ADJ\n",
            "Seq2seq --> compound --> PROPN\n",
            "model --> pobj --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "Assessing --> ROOT --> VERB\n",
            "sentence --> dobj --> NOUN\n",
            "scoring --> advcl --> VERB\n",
            "techniques --> dobj --> NOUN\n",
            "for --> prep --> ADP\n",
            "extractive --> amod --> ADJ\n",
            "text --> compound --> NOUN\n",
            "summarization --> pobj --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "The --> det --> DET\n",
            "use --> ROOT --> NOUN\n",
            "of --> prep --> ADP\n",
            "domain --> npadvmod --> NOUN\n",
            "- --> punct --> PUNCT\n",
            "specific --> amod --> ADJ\n",
            "concepts --> pobj --> NOUN\n",
            "in --> prep --> ADP\n",
            "biomedical --> amod --> ADJ\n",
            "text --> compound --> NOUN\n",
            "summarization --> pobj --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "Text --> compound --> NOUN\n",
            "summarization --> ROOT --> NOUN\n",
            "using --> acl --> VERB\n",
            "a --> det --> DET\n",
            "trainable --> amod --> ADJ\n",
            "summarizer --> nmod --> NOUN\n",
            "and --> cc --> CCONJ\n",
            "latent --> conj --> NOUN\n",
            "semantic --> amod --> ADJ\n",
            "analysis --> dobj --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "Text --> compound --> NOUN\n",
            "Summarization --> ROOT --> NOUN\n",
            "with --> prep --> ADP\n",
            "Automatic --> compound --> PROPN\n",
            "Keyword --> compound --> PROPN\n",
            "Extraction --> pobj --> PROPN\n",
            "in --> prep --> ADP\n",
            "Telugu --> compound --> PROPN\n",
            "e --> compound --> NOUN\n",
            "- --> punct --> PUNCT\n",
            "Newspapers --> pobj --> PROPN\n",
            ". --> punct --> PUNCT\n",
            "GA --> nsubj --> PROPN\n",
            ", --> punct --> PUNCT\n",
            "MR --> appos --> PROPN\n",
            ", --> punct --> PUNCT\n",
            "FFNN --> conj --> PROPN\n",
            ", --> punct --> PUNCT\n",
            "PNN --> conj --> PROPN\n",
            "and --> cc --> CCONJ\n",
            "GMM --> npadvmod --> PROPN\n",
            "based --> conj --> VERB\n",
            "models --> ROOT --> NOUN\n",
            "for --> prep --> ADP\n",
            "automatic --> amod --> ADJ\n",
            "text --> compound --> NOUN\n",
            "summarization --> pobj --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "Collaborative --> amod --> ADJ\n",
            "ranking --> npadvmod --> NOUN\n",
            "- --> punct --> PUNCT\n",
            "based --> amod --> VERB\n",
            "text --> compound --> NOUN\n",
            "summarization --> ROOT --> NOUN\n",
            "using --> acl --> VERB\n",
            "a --> det --> DET\n",
            "metaheuristic --> amod --> ADJ\n",
            "approach --> dobj --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "A --> det --> DET\n",
            "new --> amod --> ADJ\n",
            "graph --> npadvmod --> NOUN\n",
            "- --> punct --> PUNCT\n",
            "based --> amod --> VERB\n",
            "algorithm --> ROOT --> NOUN\n",
            "for --> prep --> ADP\n",
            "Persian --> amod --> ADJ\n",
            "text --> compound --> NOUN\n",
            "summarization --> pobj --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "Text --> compound --> NOUN\n",
            "summarization --> ROOT --> NOUN\n",
            "for --> prep --> ADP\n",
            "big --> amod --> ADJ\n",
            "data --> pobj --> NOUN\n",
            ": --> punct --> PUNCT\n",
            "A --> det --> DET\n",
            "comprehensive --> amod --> ADJ\n",
            "survey --> appos --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "Word --> nmod --> NOUN\n",
            "- --> punct --> PUNCT\n",
            "sentence --> compound --> NOUN\n",
            "co --> dep --> NOUN\n",
            "- --> dep --> NOUN\n",
            "ranking --> ROOT --> ADJ\n",
            "for --> prep --> ADP\n",
            "automatic --> amod --> ADJ\n",
            "extractive --> amod --> ADJ\n",
            "text --> compound --> NOUN\n",
            "summarization --> pobj --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "Responsive --> amod --> ADJ\n",
            "text --> compound --> NOUN\n",
            "summarization --> ROOT --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "A --> det --> DET\n",
            "new --> amod --> ADJ\n",
            "automatic --> amod --> ADJ\n",
            "multi --> dep --> ADJ\n",
            "- --> dep --> ADJ\n",
            "document --> amod --> ADJ\n",
            "text --> compound --> NOUN\n",
            "summarization --> ROOT --> NOUN\n",
            "using --> acl --> VERB\n",
            "topic --> compound --> NOUN\n",
            "modeling --> dobj --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "Extractive --> amod --> ADJ\n",
            "text --> compound --> NOUN\n",
            "summarization --> compound --> NOUN\n",
            "system --> ROOT --> NOUN\n",
            "to --> aux --> PART\n",
            "aid --> relcl --> VERB\n",
            "data --> compound --> NOUN\n",
            "extraction --> dobj --> NOUN\n",
            "from --> prep --> ADP\n",
            "full --> amod --> ADJ\n",
            "text --> pobj --> NOUN\n",
            "in --> prep --> ADP\n",
            "systematic --> amod --> ADJ\n",
            "review --> compound --> NOUN\n",
            "development --> pobj --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "MCMR --> ROOT --> NOUN\n",
            ": --> punct --> PUNCT\n",
            "Maximum --> amod --> ADJ\n",
            "coverage --> nmod --> NOUN\n",
            "and --> cc --> CCONJ\n",
            "minimum --> conj --> ADJ\n",
            "redundant --> amod --> ADJ\n",
            "text --> compound --> NOUN\n",
            "summarization --> compound --> NOUN\n",
            "model --> ROOT --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "PSO --> npadvmod --> NOUN\n",
            "- --> punct --> PUNCT\n",
            "Based --> amod --> VERB\n",
            "Text --> compound --> PROPN\n",
            "Summarization --> compound --> PROPN\n",
            "Approach --> ROOT --> PROPN\n",
            "Using --> acl --> PROPN\n",
            "Sentiment --> compound --> PROPN\n",
            "Analysis --> dobj --> PROPN\n",
            ". --> punct --> PUNCT\n",
            "A --> det --> DET\n",
            "multi --> dep --> ADJ\n",
            "- --> dep --> ADJ\n",
            "document --> amod --> ADJ\n",
            "summarization --> compound --> NOUN\n",
            "system --> ROOT --> NOUN\n",
            "based --> acl --> VERB\n",
            "on --> prep --> ADP\n",
            "statistics --> pobj --> NOUN\n",
            "and --> cc --> CCONJ\n",
            "linguistic --> amod --> ADJ\n",
            "treatment --> conj --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "Supporting --> ROOT --> VERB\n",
            "main --> amod --> ADJ\n",
            "idea --> compound --> NOUN\n",
            "identification --> dobj --> NOUN\n",
            "and --> cc --> CCONJ\n",
            "text --> compound --> NOUN\n",
            "summarization --> conj --> NOUN\n",
            "in --> prep --> ADP\n",
            "middle --> amod --> ADJ\n",
            "school --> pobj --> NOUN\n",
            "co --> dep --> NOUN\n",
            "- --> dep --> VERB\n",
            "taught --> ROOT --> ADJ\n",
            "classes --> dobj --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "COMPENDIUM --> ROOT --> NOUN\n",
            ": --> punct --> PUNCT\n",
            "A --> det --> DET\n",
            "text --> compound --> NOUN\n",
            "summarization --> compound --> NOUN\n",
            "system --> ROOT --> NOUN\n",
            "for --> prep --> ADP\n",
            "generating --> pcomp --> VERB\n",
            "abstracts --> dobj --> NOUN\n",
            "of --> prep --> ADP\n",
            "research --> compound --> NOUN\n",
            "papers --> pobj --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "An --> det --> DET\n",
            "improvised --> amod --> ADJ\n",
            "extractive --> amod --> ADJ\n",
            "approach --> ROOT --> NOUN\n",
            "to --> aux --> ADP\n",
            "hindi --> acl --> PROPN\n",
            "text --> compound --> PROPN\n",
            "summarization --> dobj --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "A --> det --> DET\n",
            "new --> amod --> ADJ\n",
            "sentence --> compound --> NOUN\n",
            "similarity --> compound --> NOUN\n",
            "measure --> ROOT --> NOUN\n",
            "and --> cc --> CCONJ\n",
            "sentence --> npadvmod --> NOUN\n",
            "based --> amod --> VERB\n",
            "extractive --> amod --> ADJ\n",
            "technique --> conj --> NOUN\n",
            "for --> prep --> ADP\n",
            "automatic --> amod --> ADJ\n",
            "text --> compound --> NOUN\n",
            "summarization --> pobj --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "Text --> compound --> NOUN\n",
            "summarization --> ROOT --> NOUN\n",
            "using --> acl --> VERB\n",
            "unsupervised --> amod --> VERB\n",
            "deep --> amod --> ADJ\n",
            "learning --> dobj --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "Text --> compound --> NOUN\n",
            "summarization --> ROOT --> NOUN\n",
            "in --> prep --> ADP\n",
            "the --> det --> DET\n",
            "biomedical --> amod --> ADJ\n",
            "domain --> pobj --> NOUN\n",
            ": --> punct --> PUNCT\n",
            "a --> det --> DET\n",
            "systematic --> amod --> ADJ\n",
            "review --> appos --> NOUN\n",
            "of --> prep --> ADP\n",
            "recent --> amod --> ADJ\n",
            "research --> pobj --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "Abstractive --> amod --> ADJ\n",
            "text --> compound --> NOUN\n",
            "summarization --> nsubj --> NOUN\n",
            "using --> acl --> VERB\n",
            "LSTM --> compound --> PROPN\n",
            "- --> punct --> PUNCT\n",
            "CNN --> dobj --> PROPN\n",
            "based --> ROOT --> VERB\n",
            "deep --> amod --> ADJ\n",
            "learning --> dobj --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "Karcı --> compound --> PROPN\n",
            "summarization --> ROOT --> NOUN\n",
            ": --> punct --> PUNCT\n",
            "A --> det --> DET\n",
            "simple --> amod --> ADJ\n",
            "and --> cc --> CCONJ\n",
            "effective --> conj --> ADJ\n",
            "approach --> ROOT --> NOUN\n",
            "for --> prep --> ADP\n",
            "automatic --> amod --> ADJ\n",
            "text --> compound --> NOUN\n",
            "summarization --> pobj --> NOUN\n",
            "using --> acl --> VERB\n",
            "Karcı --> compound --> PROPN\n",
            "entropy --> dobj --> ADJ\n",
            ". --> punct --> PUNCT\n",
            "Deep --> amod --> ADJ\n",
            "learning --> ROOT --> NOUN\n",
            "in --> prep --> ADP\n",
            "the --> det --> DET\n",
            "domain --> pobj --> NOUN\n",
            "of --> prep --> ADP\n",
            "multi --> dep --> ADJ\n",
            "- --> dep --> ADJ\n",
            "document --> amod --> ADJ\n",
            "text --> compound --> NOUN\n",
            "summarization --> pobj --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "Selection --> compound --> NOUN\n",
            "criteria --> ROOT --> NOUN\n",
            "for --> prep --> ADP\n",
            "text --> compound --> NOUN\n",
            "mining --> compound --> NOUN\n",
            "approaches --> pobj --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "A --> det --> DET\n",
            "modification --> ROOT --> NOUN\n",
            "to --> prep --> ADP\n",
            "graph --> npadvmod --> NOUN\n",
            "based --> amod --> VERB\n",
            "approach --> pobj --> NOUN\n",
            "for --> prep --> ADP\n",
            "extraction --> pobj --> NOUN\n",
            "based --> acl --> VERB\n",
            "automatic --> amod --> ADJ\n",
            "text --> compound --> NOUN\n",
            "summarization --> dobj --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "A --> det --> DET\n",
            "text --> compound --> NOUN\n",
            "summarization --> compound --> NOUN\n",
            "method --> ROOT --> NOUN\n",
            "based --> acl --> VERB\n",
            "on --> prep --> ADP\n",
            "fuzzy --> amod --> ADJ\n",
            "rules --> pobj --> NOUN\n",
            "and --> cc --> CCONJ\n",
            "applicable --> conj --> ADJ\n",
            "to --> prep --> ADP\n",
            "automated --> amod --> VERB\n",
            "assessment --> pobj --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "Automatic --> amod --> ADJ\n",
            "Text --> compound --> PROPN\n",
            "Summarization --> ROOT --> NOUN\n",
            "Using --> acl --> VERB\n",
            "Fuzzy --> compound --> PROPN\n",
            "Extraction --> dobj --> PROPN\n",
            ". --> punct --> PUNCT\n",
            "A --> det --> DET\n",
            "complex --> amod --> ADJ\n",
            "network --> compound --> NOUN\n",
            "approach --> ROOT --> NOUN\n",
            "to --> prep --> ADP\n",
            "text --> compound --> NOUN\n",
            "summarization --> pobj --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "Text --> compound --> NOUN\n",
            "summarization --> ROOT --> NOUN\n",
            "based --> acl --> VERB\n",
            "on --> prep --> ADP\n",
            "classification --> pobj --> NOUN\n",
            "using --> acl --> VERB\n",
            "ANFIS --> dobj --> PROPN\n",
            ". --> punct --> PUNCT\n",
            "Text --> compound --> NOUN\n",
            "summarization --> ROOT --> NOUN\n",
            "from --> prep --> ADP\n",
            "legal --> amod --> ADJ\n",
            "documents --> pobj --> NOUN\n",
            ": --> punct --> PUNCT\n",
            "a --> det --> DET\n",
            "survey --> appos --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "Text --> compound --> NOUN\n",
            "document --> compound --> NOUN\n",
            "summarization --> ROOT --> NOUN\n",
            "using --> acl --> VERB\n",
            "word --> compound --> NOUN\n",
            "embedding --> dobj --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "Graph --> npadvmod --> PROPN\n",
            "based --> amod --> VERB\n",
            "technique --> ROOT --> NOUN\n",
            "for --> prep --> ADP\n",
            "hindi --> compound --> PROPN\n",
            "text --> compound --> PROPN\n",
            "summarization --> pobj --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "Extraction --> npadvmod --> NOUN\n",
            "based --> ROOT --> VERB\n",
            "text --> compound --> NOUN\n",
            "summarization --> compound --> NOUN\n",
            "methods --> dobj --> NOUN\n",
            "on --> prep --> ADP\n",
            "user --> poss --> NOUN\n",
            "'s --> case --> PART\n",
            "review --> compound --> NOUN\n",
            "data --> pobj --> NOUN\n",
            ": --> punct --> PUNCT\n",
            "A --> det --> DET\n",
            "comparative --> amod --> ADJ\n",
            "study --> ROOT --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "Opinion --> compound --> NOUN\n",
            "mining --> ROOT --> NOUN\n",
            "from --> prep --> ADP\n",
            "online --> amod --> ADJ\n",
            "hotel --> compound --> NOUN\n",
            "reviews --> pobj --> NOUN\n",
            "– --> punct --> PUNCT\n",
            "a --> det --> DET\n",
            "text --> compound --> NOUN\n",
            "summarization --> compound --> NOUN\n",
            "approach --> appos --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "The --> det --> DET\n",
            "effectiveness --> ROOT --> NOUN\n",
            "of --> prep --> ADP\n",
            "automatic --> amod --> ADJ\n",
            "text --> compound --> NOUN\n",
            "summarization --> pobj --> NOUN\n",
            "in --> prep --> ADP\n",
            "mobile --> amod --> ADJ\n",
            "learning --> compound --> VERB\n",
            "contexts --> pobj --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "Recent --> amod --> ADJ\n",
            "automatic --> amod --> ADJ\n",
            "text --> compound --> NOUN\n",
            "summarization --> compound --> NOUN\n",
            "techniques --> ROOT --> NOUN\n",
            ": --> punct --> PUNCT\n",
            "a --> det --> DET\n",
            "survey --> appos --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "Deep --> advmod --> ADJ\n",
            "contextualized --> ROOT --> VERB\n",
            "embeddings --> dobj --> NOUN\n",
            "for --> prep --> ADP\n",
            "quantifying --> pcomp --> VERB\n",
            "the --> det --> DET\n",
            "informative --> amod --> ADJ\n",
            "content --> dobj --> NOUN\n",
            "in --> prep --> ADP\n",
            "biomedical --> amod --> ADJ\n",
            "text --> compound --> NOUN\n",
            "summarization --> pobj --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "How --> advmod --> ADV\n",
            "to --> aux --> PART\n",
            "improve --> ROOT --> VERB\n",
            "text --> compound --> NOUN\n",
            "summarization --> dobj --> NOUN\n",
            "and --> cc --> CCONJ\n",
            "classification --> conj --> NOUN\n",
            "by --> prep --> ADP\n",
            "mutual --> amod --> ADJ\n",
            "cooperation --> pobj --> NOUN\n",
            "on --> prep --> ADP\n",
            "an --> det --> DET\n",
            "integrated --> amod --> ADJ\n",
            "framework --> pobj --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "Figure --> npadvmod --> NOUN\n",
            "- --> punct --> PUNCT\n",
            "associated --> amod --> VERB\n",
            "text --> compound --> NOUN\n",
            "summarization --> ROOT --> NOUN\n",
            "and --> cc --> CCONJ\n",
            "evaluation --> conj --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "Extractive --> amod --> ADJ\n",
            "multi --> dep --> ADJ\n",
            "- --> dep --> ADJ\n",
            "document --> amod --> ADJ\n",
            "text --> compound --> NOUN\n",
            "summarization --> ROOT --> NOUN\n",
            "using --> acl --> VERB\n",
            "a --> det --> DET\n",
            "multi --> dep --> ADJ\n",
            "- --> dep --> ADJ\n",
            "objective --> amod --> ADJ\n",
            "artificial --> amod --> ADJ\n",
            "bee --> compound --> PROPN\n",
            "colony --> compound --> NOUN\n",
            "optimization --> compound --> NOUN\n",
            "approach --> dobj --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "CRHASum --> dep --> NOUN\n",
            ": --> punct --> PUNCT\n",
            "extractive --> amod --> ADJ\n",
            "text --> compound --> NOUN\n",
            "summarization --> ROOT --> NOUN\n",
            "with --> prep --> ADP\n",
            "contextualized --> amod --> VERB\n",
            "- --> punct --> PUNCT\n",
            "representation --> nmod --> NOUN\n",
            "hierarchical --> amod --> ADJ\n",
            "- --> punct --> PUNCT\n",
            "attention --> compound --> NOUN\n",
            "summarization --> compound --> NOUN\n",
            "network --> pobj --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "Literature --> compound --> NOUN\n",
            "study --> ROOT --> NOUN\n",
            "on --> prep --> ADP\n",
            "multi --> dep --> ADJ\n",
            "- --> dep --> ADJ\n",
            "document --> amod --> ADJ\n",
            "text --> compound --> NOUN\n",
            "summarization --> compound --> NOUN\n",
            "techniques --> pobj --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "A --> det --> DET\n",
            "proposed --> amod --> VERB\n",
            "methodology --> ROOT --> NOUN\n",
            "for --> prep --> ADP\n",
            "subjective --> amod --> ADJ\n",
            "evaluation --> pobj --> NOUN\n",
            "of --> prep --> ADP\n",
            "video --> nmod --> NOUN\n",
            "and --> cc --> CCONJ\n",
            "text --> conj --> NOUN\n",
            "summarization --> pobj --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "A --> det --> DET\n",
            "Bengali --> compound --> PROPN\n",
            "text --> compound --> NOUN\n",
            "generation --> compound --> NOUN\n",
            "approach --> ROOT --> NOUN\n",
            "in --> prep --> ADP\n",
            "context --> pobj --> NOUN\n",
            "of --> prep --> ADP\n",
            "abstractive --> amod --> ADJ\n",
            "text --> compound --> NOUN\n",
            "summarization --> pobj --> NOUN\n",
            "using --> acl --> VERB\n",
            "rnn --> dobj --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "A --> det --> DET\n",
            "two --> nummod --> NUM\n",
            "- --> punct --> PUNCT\n",
            "stage --> nmod --> NOUN\n",
            "Chinese --> amod --> ADJ\n",
            "text --> compound --> NOUN\n",
            "summarization --> compound --> NOUN\n",
            "algorithm --> ROOT --> NOUN\n",
            "using --> acl --> VERB\n",
            "keyword --> compound --> NOUN\n",
            "information --> dobj --> NOUN\n",
            "and --> cc --> CCONJ\n",
            "adversarial --> amod --> ADJ\n",
            "learning --> conj --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "A --> det --> DET\n",
            "comprehensive --> amod --> ADJ\n",
            "survey --> ROOT --> NOUN\n",
            "on --> prep --> ADP\n",
            "extractive --> amod --> ADJ\n",
            "and --> cc --> CCONJ\n",
            "abstractive --> conj --> ADJ\n",
            "techniques --> pobj --> NOUN\n",
            "for --> prep --> ADP\n",
            "text --> compound --> NOUN\n",
            "summarization --> pobj --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "ATSSC --> ROOT --> PROPN\n",
            ": --> punct --> PUNCT\n",
            "Development --> appos --> NOUN\n",
            "of --> prep --> ADP\n",
            "an --> det --> DET\n",
            "approach --> pobj --> NOUN\n",
            "based --> acl --> VERB\n",
            "on --> prep --> ADP\n",
            "soft --> amod --> ADJ\n",
            "computing --> pobj --> NOUN\n",
            "for --> prep --> ADP\n",
            "text --> compound --> NOUN\n",
            "summarization --> pobj --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "Text --> compound --> NOUN\n",
            "Summarization --> ROOT --> NOUN\n",
            ": --> punct --> PUNCT\n",
            "An --> det --> DET\n",
            "Extractive --> compound --> PROPN\n",
            "Approach --> ROOT --> PROPN\n",
            ". --> punct --> PUNCT\n",
            "A --> det --> DET\n",
            "variable --> amod --> ADJ\n",
            "dimension --> compound --> NOUN\n",
            "optimization --> compound --> NOUN\n",
            "approach --> ROOT --> NOUN\n",
            "for --> prep --> ADP\n",
            "text --> compound --> NOUN\n",
            "summarization --> pobj --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "Multidocument --> amod --> ADJ\n",
            "arabic --> amod --> ADJ\n",
            "text --> compound --> NOUN\n",
            "summarization --> ROOT --> NOUN\n",
            "based --> acl --> VERB\n",
            "on --> prep --> ADP\n",
            "clustering --> pcomp --> VERB\n",
            "and --> cc --> CCONJ\n",
            "Word2Vec --> conj --> NOUN\n",
            "to --> aux --> PART\n",
            "reduce --> conj --> VERB\n",
            "redundancy --> dobj --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "Extractive --> amod --> ADJ\n",
            "summarization --> ROOT --> NOUN\n",
            "of --> prep --> ADP\n",
            "a --> det --> DET\n",
            "document --> pobj --> NOUN\n",
            "using --> acl --> VERB\n",
            "lexical --> amod --> ADJ\n",
            "chains --> dobj --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "Comparison --> ROOT --> NOUN\n",
            "of --> prep --> ADP\n",
            "PubMed --> pobj --> PROPN\n",
            ", --> punct --> PUNCT\n",
            "Scopus --> conj --> PROPN\n",
            ", --> punct --> PUNCT\n",
            "web --> appos --> NOUN\n",
            "of --> prep --> ADP\n",
            "science --> pobj --> NOUN\n",
            ", --> punct --> PUNCT\n",
            "and --> cc --> CCONJ\n",
            "Google --> compound --> PROPN\n",
            "scholar --> conj --> NOUN\n",
            ": --> punct --> PUNCT\n",
            "strengths --> appos --> NOUN\n",
            "and --> cc --> CCONJ\n",
            "weaknesses --> conj --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "Fuzzy --> amod --> ADJ\n",
            "swarm --> nsubj --> ADJ\n",
            "diversity --> ROOT --> NOUN\n",
            "hybrid --> amod --> NOUN\n",
            "model --> dobj --> NOUN\n",
            "for --> prep --> ADP\n",
            "text --> compound --> NOUN\n",
            "summarization --> pobj --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "Speech --> nmod --> NOUN\n",
            "- --> punct --> PUNCT\n",
            "to --> prep --> ADP\n",
            "- --> punct --> PUNCT\n",
            "text --> pobj --> NOUN\n",
            "summarization --> ROOT --> NOUN\n",
            "using --> acl --> VERB\n",
            "automatic --> amod --> ADJ\n",
            "phrase --> compound --> NOUN\n",
            "extraction --> dobj --> NOUN\n",
            "from --> prep --> ADP\n",
            "recognized --> amod --> VERB\n",
            "text --> pobj --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "Rulingbr --> ROOT --> NOUN\n",
            ": --> punct --> PUNCT\n",
            "A --> det --> DET\n",
            "summarization --> nsubj --> NOUN\n",
            "dataset --> ROOT --> VERB\n",
            "for --> prep --> ADP\n",
            "legal --> amod --> ADJ\n",
            "texts --> pobj --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "Application --> ROOT --> NOUN\n",
            "of --> prep --> ADP\n",
            "Extractive --> compound --> PROPN\n",
            "Text --> compound --> PROPN\n",
            "Summarization --> compound --> PROPN\n",
            "Algorithms --> pobj --> PROPN\n",
            "to --> prep --> ADP\n",
            "Speech --> nmod --> NOUN\n",
            "- --> punct --> PUNCT\n",
            "to --> prep --> ADP\n",
            "- --> punct --> PUNCT\n",
            "Text --> pobj --> PROPN\n",
            "Media --> pobj --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "Text --> compound --> NOUN\n",
            "summarization --> ROOT --> NOUN\n",
            "using --> acl --> VERB\n",
            "Wikipedia --> dobj --> PROPN\n",
            ". --> punct --> PUNCT\n",
            "SRL --> compound --> PROPN\n",
            "- --> punct --> PUNCT\n",
            "ESA --> compound --> PROPN\n",
            "- --> punct --> PUNCT\n",
            "TextSum --> ROOT --> PROPN\n",
            ": --> punct --> PUNCT\n",
            "A --> det --> DET\n",
            "text --> compound --> NOUN\n",
            "summarization --> compound --> NOUN\n",
            "approach --> ROOT --> NOUN\n",
            "based --> acl --> VERB\n",
            "on --> prep --> ADP\n",
            "semantic --> amod --> ADJ\n",
            "role --> compound --> NOUN\n",
            "labeling --> pobj --> NOUN\n",
            "and --> cc --> CCONJ\n",
            "explicit --> amod --> ADJ\n",
            "semantic --> amod --> ADJ\n",
            "analysis --> conj --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "A --> det --> DET\n",
            "multilingual --> amod --> ADJ\n",
            "study --> ROOT --> NOUN\n",
            "of --> prep --> ADP\n",
            "compressive --> amod --> ADJ\n",
            "cross --> dep --> ADJ\n",
            "- --> dep --> ADJ\n",
            "language --> compound --> ADJ\n",
            "text --> compound --> NOUN\n",
            "summarization --> pobj --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "A --> det --> DET\n",
            "Study --> ROOT --> PROPN\n",
            "of --> prep --> ADP\n",
            "Text --> compound --> PROPN\n",
            "Summarization --> compound --> NOUN\n",
            "Techniques --> pobj --> NOUN\n",
            "for --> prep --> ADP\n",
            "Generating --> compound --> PROPN\n",
            "Meeting --> compound --> PROPN\n",
            "Minutes --> pobj --> PROPN\n",
            ". --> punct --> PUNCT\n",
            "Minimum --> amod --> ADJ\n",
            "redundancy --> ROOT --> NOUN\n",
            "and --> cc --> CCONJ\n",
            "maximum --> amod --> ADJ\n",
            "relevance --> conj --> NOUN\n",
            "for --> prep --> ADP\n",
            "single --> amod --> ADJ\n",
            "and --> cc --> CCONJ\n",
            "multi --> dep --> ADJ\n",
            "- --> dep --> ADJ\n",
            "document --> conj --> ADJ\n",
            "Arabic --> amod --> ADJ\n",
            "text --> compound --> NOUN\n",
            "summarization --> pobj --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "Extractive --> amod --> ADJ\n",
            "summarization --> ROOT --> NOUN\n",
            "of --> prep --> ADP\n",
            "clinical --> amod --> ADJ\n",
            "trial --> compound --> NOUN\n",
            "descriptions --> pobj --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "Using --> ROOT --> VERB\n",
            "query --> compound --> NOUN\n",
            "expansion --> dobj --> NOUN\n",
            "in --> prep --> ADP\n",
            "graph --> npadvmod --> NOUN\n",
            "- --> punct --> PUNCT\n",
            "based --> amod --> VERB\n",
            "approach --> pobj --> NOUN\n",
            "for --> prep --> ADP\n",
            "query --> npadvmod --> PROPN\n",
            "- --> punct --> PUNCT\n",
            "focused --> amod --> VERB\n",
            "multi --> dep --> ADJ\n",
            "- --> dep --> ADJ\n",
            "document --> amod --> ADJ\n",
            "summarization --> pobj --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "Extractive --> amod --> ADJ\n",
            "arabic --> amod --> ADJ\n",
            "text --> compound --> NOUN\n",
            "summarization --> ROOT --> NOUN\n",
            "using --> acl --> VERB\n",
            "modified --> amod --> VERB\n",
            "PageRank --> compound --> PROPN\n",
            "algorithm --> dobj --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "Evaluating --> ROOT --> VERB\n",
            "different --> amod --> ADJ\n",
            "similarity --> compound --> NOUN\n",
            "measures --> dobj --> NOUN\n",
            "for --> prep --> ADP\n",
            "automatic --> amod --> ADJ\n",
            "biomedical --> amod --> ADJ\n",
            "text --> compound --> NOUN\n",
            "summarization --> pobj --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "Hybrid --> amod --> ADJ\n",
            "text --> compound --> NOUN\n",
            "summarization --> ROOT --> NOUN\n",
            ": --> punct --> PUNCT\n",
            "a --> det --> DET\n",
            "survey --> appos --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "Semantic --> amod --> ADJ\n",
            "Graph --> ROOT --> PROPN\n",
            "Based --> ROOT --> PROPN\n",
            "Automatic --> compound --> PROPN\n",
            "Text --> compound --> PROPN\n",
            "Summarization --> dobj --> PROPN\n",
            "for --> prep --> ADP\n",
            "Hindi --> compound --> PROPN\n",
            "Documents --> pobj --> PROPN\n",
            "Using --> acl --> VERB\n",
            "Particle --> compound --> PROPN\n",
            "Swarm --> compound --> PROPN\n",
            "Optimization --> dobj --> PROPN\n",
            ". --> punct --> PUNCT\n",
            "Towards --> prep --> ADP\n",
            "automatic --> amod --> ADJ\n",
            "tweet --> compound --> ADJ\n",
            "generation --> pobj --> NOUN\n",
            ": --> punct --> PUNCT\n",
            "A --> det --> DET\n",
            "comparative --> amod --> ADJ\n",
            "study --> ROOT --> NOUN\n",
            "from --> prep --> ADP\n",
            "the --> det --> DET\n",
            "text --> compound --> NOUN\n",
            "summarization --> compound --> NOUN\n",
            "perspective --> pobj --> NOUN\n",
            "in --> prep --> ADP\n",
            "the --> det --> DET\n",
            "journalism --> compound --> NOUN\n",
            "genre --> pobj --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "Abstractive --> amod --> ADJ\n",
            "multi --> dep --> ADJ\n",
            "- --> dep --> ADJ\n",
            "document --> amod --> ADJ\n",
            "text --> compound --> NOUN\n",
            "summarization --> ROOT --> NOUN\n",
            "using --> acl --> VERB\n",
            "a --> det --> DET\n",
            "genetic --> amod --> ADJ\n",
            "algorithm --> dobj --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "Long --> amod --> ADV\n",
            "- --> punct --> PUNCT\n",
            "span --> compound --> NOUN\n",
            "language --> compound --> NOUN\n",
            "models --> ROOT --> NOUN\n",
            "for --> prep --> ADP\n",
            "query --> npadvmod --> NOUN\n",
            "- --> punct --> PUNCT\n",
            "focused --> amod --> VERB\n",
            "unsupervised --> amod --> VERB\n",
            "extractive --> amod --> ADJ\n",
            "text --> compound --> NOUN\n",
            "summarization --> pobj --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "An --> det --> DET\n",
            "improved --> amod --> ADJ\n",
            "method --> ROOT --> NOUN\n",
            "of --> prep --> ADP\n",
            "automatic --> amod --> ADJ\n",
            "text --> compound --> NOUN\n",
            "summarization --> pobj --> NOUN\n",
            "for --> prep --> ADP\n",
            "web --> compound --> NOUN\n",
            "contents --> pobj --> NOUN\n",
            "using --> acl --> VERB\n",
            "lexical --> amod --> ADJ\n",
            "chain --> dobj --> NOUN\n",
            "with --> prep --> ADP\n",
            "semantic --> amod --> ADJ\n",
            "- --> punct --> PUNCT\n",
            "related --> amod --> VERB\n",
            "terms --> pobj --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "Cross --> dep --> ADJ\n",
            "- --> dep --> ADJ\n",
            "language --> amod --> NOUN\n",
            "text --> compound --> NOUN\n",
            "summarization --> ROOT --> NOUN\n",
            "using --> acl --> VERB\n",
            "sentence --> dobj --> NOUN\n",
            "and --> cc --> CCONJ\n",
            "multi --> dep --> ADJ\n",
            "- --> dep --> ADJ\n",
            "sentence --> amod --> ADJ\n",
            "compression --> conj --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "Extractive --> amod --> ADJ\n",
            "multi --> dep --> ADJ\n",
            "- --> dep --> ADJ\n",
            "document --> amod --> ADJ\n",
            "text --> compound --> NOUN\n",
            "summarization --> ROOT --> NOUN\n",
            "based --> acl --> VERB\n",
            "on --> prep --> ADP\n",
            "graph --> compound --> NOUN\n",
            "independent --> amod --> ADJ\n",
            "sets --> pobj --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "Cross --> dep --> ADJ\n",
            "- --> dep --> PUNCT\n",
            "lingual --> amod --> ADJ\n",
            "speech --> nmod --> NOUN\n",
            "- --> punct --> PUNCT\n",
            "to --> prep --> ADP\n",
            "- --> punct --> PUNCT\n",
            "text --> pobj --> NOUN\n",
            "summarization --> ROOT --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "Abstractive --> amod --> ADJ\n",
            "social --> amod --> ADJ\n",
            "media --> compound --> NOUN\n",
            "text --> compound --> NOUN\n",
            "summarization --> ROOT --> NOUN\n",
            "using --> acl --> VERB\n",
            "selective --> amod --> ADJ\n",
            "reinforced --> amod --> VERB\n",
            "Seq2Seq --> compound --> PROPN\n",
            "attention --> compound --> NOUN\n",
            "model --> dobj --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "A --> det --> DET\n",
            "hybrid --> amod --> ADJ\n",
            "approach --> ROOT --> NOUN\n",
            "for --> prep --> ADP\n",
            "arabic --> amod --> ADJ\n",
            "text --> compound --> NOUN\n",
            "summarization --> pobj --> NOUN\n",
            "using --> acl --> VERB\n",
            "domain --> compound --> NOUN\n",
            "knowledge --> dobj --> NOUN\n",
            "and --> cc --> CCONJ\n",
            "genetic --> amod --> ADJ\n",
            "algorithms --> conj --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "Abstractive --> amod --> ADJ\n",
            "text --> compound --> NOUN\n",
            "summarization --> ROOT --> NOUN\n",
            "based --> acl --> VERB\n",
            "on --> prep --> ADP\n",
            "improved --> amod --> VERB\n",
            "semantic --> amod --> ADJ\n",
            "graph --> compound --> NOUN\n",
            "approach --> pobj --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "Comparative --> amod --> ADJ\n",
            "study --> ROOT --> NOUN\n",
            "of --> prep --> ADP\n",
            "DE --> pobj --> NOUN\n",
            "and --> cc --> CCONJ\n",
            "PSO --> conj --> PROPN\n",
            "over --> prep --> ADP\n",
            "document --> compound --> NOUN\n",
            "summarization --> pobj --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "Facilitating --> ROOT --> VERB\n",
            "physicians --> poss --> NOUN\n",
            "' --> case --> PART\n",
            "access --> dobj --> NOUN\n",
            "to --> prep --> ADP\n",
            "information --> pobj --> NOUN\n",
            "via --> prep --> ADP\n",
            "tailored --> amod --> VERB\n",
            "text --> compound --> NOUN\n",
            "summarization --> pobj --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "A --> det --> DET\n",
            "proposed --> amod --> VERB\n",
            "natural --> amod --> ADJ\n",
            "language --> compound --> NOUN\n",
            "processing --> compound --> NOUN\n",
            "preprocessing --> compound --> NOUN\n",
            "procedures --> ROOT --> NOUN\n",
            "for --> prep --> ADP\n",
            "enhancing --> pcomp --> VERB\n",
            "arabic --> amod --> ADJ\n",
            "text --> compound --> NOUN\n",
            "summarization --> dobj --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "Different --> amod --> ADJ\n",
            "approaches --> ROOT --> NOUN\n",
            "for --> prep --> ADP\n",
            "identifying --> pcomp --> VERB\n",
            "important --> amod --> ADJ\n",
            "concepts --> dobj --> NOUN\n",
            "in --> prep --> ADP\n",
            "probabilistic --> amod --> ADJ\n",
            "biomedical --> amod --> ADJ\n",
            "text --> compound --> NOUN\n",
            "summarization --> pobj --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "Extractive --> compound --> ADJ\n",
            "Text --> compound --> PROPN\n",
            "Summarization --> compound --> PROPN\n",
            "Models --> ROOT --> PROPN\n",
            "for --> prep --> ADP\n",
            "Urdu --> compound --> PROPN\n",
            "Language --> pobj --> PROPN\n",
            ". --> punct --> PUNCT\n",
            "Analysis --> ROOT --> NOUN\n",
            "of --> prep --> ADP\n",
            "competitor --> compound --> NOUN\n",
            "intelligence --> pobj --> NOUN\n",
            "in --> prep --> ADP\n",
            "the --> det --> DET\n",
            "era --> pobj --> NOUN\n",
            "of --> prep --> ADP\n",
            "big --> amod --> ADJ\n",
            "data --> pobj --> NOUN\n",
            ": --> punct --> PUNCT\n",
            "an --> det --> DET\n",
            "integrated --> amod --> VERB\n",
            "system --> appos --> NOUN\n",
            "using --> acl --> VERB\n",
            "text --> compound --> NOUN\n",
            "summarization --> dobj --> NOUN\n",
            "based --> acl --> VERB\n",
            "on --> prep --> ADP\n",
            "global --> amod --> ADJ\n",
            "optimization --> pobj --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "Text --> compound --> NOUN\n",
            "summarization --> ROOT --> NOUN\n",
            "using --> acl --> VERB\n",
            "wordnet --> compound --> NOUN\n",
            "graph --> compound --> NOUN\n",
            "based --> amod --> VERB\n",
            "sentence --> compound --> NOUN\n",
            "ranking --> dobj --> VERB\n",
            ". --> punct --> PUNCT\n",
            "Corpus --> npadvmod --> PROPN\n",
            "- --> punct --> PUNCT\n",
            "based --> amod --> VERB\n",
            "problem --> compound --> NOUN\n",
            "selection --> ROOT --> NOUN\n",
            "for --> prep --> ADP\n",
            "EHR --> compound --> NOUN\n",
            "note --> compound --> NOUN\n",
            "summarization --> pobj --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "Summarization --> ROOT --> NOUN\n",
            "of --> prep --> ADP\n",
            "legal --> amod --> ADJ\n",
            "judgments --> pobj --> NOUN\n",
            "using --> acl --> VERB\n",
            "gravitational --> amod --> ADJ\n",
            "search --> compound --> NOUN\n",
            "algorithm --> dobj --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "Text --> compound --> NOUN\n",
            "summarization --> ROOT --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "Hybrid --> amod --> ADJ\n",
            "latent --> nmod --> NOUN\n",
            "semantic --> amod --> ADJ\n",
            "analysis --> ROOT --> NOUN\n",
            "and --> cc --> CCONJ\n",
            "random --> amod --> ADJ\n",
            "indexing --> compound --> NOUN\n",
            "model --> conj --> NOUN\n",
            "for --> prep --> ADP\n",
            "text --> compound --> NOUN\n",
            "summarization --> pobj --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "Sumdoc --> ROOT --> PROPN\n",
            ": --> punct --> PUNCT\n",
            "a --> det --> DET\n",
            "unified --> amod --> ADJ\n",
            "approach --> appos --> NOUN\n",
            "for --> prep --> ADP\n",
            "automatic --> amod --> ADJ\n",
            "text --> compound --> NOUN\n",
            "summarization --> pobj --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "Improving --> ROOT --> VERB\n",
            "text --> compound --> NOUN\n",
            "summarization --> dobj --> NOUN\n",
            "of --> prep --> ADP\n",
            "online --> amod --> ADJ\n",
            "hotel --> compound --> NOUN\n",
            "reviews --> pobj --> NOUN\n",
            "with --> prep --> ADP\n",
            "review --> compound --> NOUN\n",
            "helpfulness --> pobj --> NOUN\n",
            "and --> cc --> CCONJ\n",
            "sentiment --> conj --> NOUN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ST2DmEmB0ocb"
      },
      "source": [
        "### Methods/Algorithms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JglWMZDHzz-p",
        "outputId": "1f688f87-2783-460f-f878-31efa48e5f0b"
      },
      "source": [
        "pattern1 = [{'POS':'NOUN'}, \n",
        "           {'LOWER': 'using'},\n",
        "            {'POS':'ADJ'}]\n",
        "pattern2 = [{'POS':'NOUN'}, \n",
        "           {'LOWER': 'approach'}]\n",
        "pattern3 = [{'LOWER':'based'},\n",
        "           {'POS':'NOUN'}]\n",
        "\n",
        "\n",
        "matcher = Matcher(nlp.vocab) \n",
        "matcher.add(\"matching_1\",[pattern1]) \n",
        "matcher.add(\"matching_2\",[pattern2])\n",
        "matcher.add(\"matching_3\",[pattern3])\n",
        "matches = matcher(doc) \n",
        "for match_id, start, end in matches:\n",
        "    string_id = nlp.vocab.strings[match_id]\n",
        "    span = doc[start:end]  \n",
        "    print(match_id, string_id, start, end, span.text)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11840699188806025751 matching_1 12 15 technique using contextual\n",
            "12044629620993336498 matching_3 35 37 based text\n",
            "11840699188806025751 matching_1 47 50 summarization using deep\n",
            "10575872351855955709 matching_2 77 79 summarization approach\n",
            "12044629620993336498 matching_3 142 144 based models\n",
            "12044629620993336498 matching_3 152 154 based text\n",
            "12044629620993336498 matching_3 164 166 based algorithm\n",
            "12044629620993336498 matching_3 382 384 based approach\n",
            "10575872351855955709 matching_2 414 416 network approach\n",
            "12044629620993336498 matching_3 445 447 based technique\n",
            "12044629620993336498 matching_3 453 455 based text\n",
            "10575872351855955709 matching_2 476 478 summarization approach\n",
            "10575872351855955709 matching_2 549 551 optimization approach\n",
            "10575872351855955709 matching_2 592 594 generation approach\n",
            "10575872351855955709 matching_2 654 656 optimization approach\n",
            "11840699188806025751 matching_1 677 680 document using lexical\n",
            "11840699188806025751 matching_1 714 717 summarization using automatic\n",
            "10575872351855955709 matching_2 759 761 summarization approach\n",
            "12044629620993336498 matching_3 821 823 based approach\n",
            "11840699188806025751 matching_1 924 927 contents using lexical\n",
            "11840699188806025751 matching_1 973 976 summarization using selective\n",
            "10575872351855955709 matching_2 1002 1004 graph approach\n",
            "12044629620993336498 matching_3 1086 1088 based sentence\n",
            "12044629620993336498 matching_3 1092 1094 based problem\n",
            "11840699188806025751 matching_1 1103 1106 judgments using gravitational\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QK4ULiH0t3Y"
      },
      "source": [
        "### Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7ARIHhmzz-p",
        "outputId": "5d449de9-3a44-4471-8c0a-d30ec2e118e1"
      },
      "source": [
        "pattern1 = [{'LOWER': 'survey'},\n",
        "            {'POS':'NOUN'}]\n",
        "\n",
        "pattern2 = [{'POS':'ADJ'}, \n",
        "           {'LOWER': 'text'}]\n",
        "\n",
        "pattern3 = [{'POS':'ADJ'},\n",
        "           {'LOWER':'document'},]\n",
        "\n",
        "pattern4 = [{'POS':'ADJ'},\n",
        "           {'LOWER':'study'}]\n",
        "\n",
        "matcher = Matcher(nlp.vocab) \n",
        "matcher.add(\"matching_1\",[pattern1]) \n",
        "matcher.add(\"matching_2\",[pattern2])\n",
        "matcher.add(\"matching_3\",[pattern3])\n",
        "matcher.add(\"matching_4\",[pattern4])\n",
        "matches = matcher(doc) \n",
        "for match_id, start, end in matches:\n",
        "    string_id = nlp.vocab.strings[match_id]\n",
        "    span = doc[start:end]  \n",
        "    print(match_id, string_id, start, end, span.text)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10575872351855955709 matching_2 23 25 Automatic text\n",
            "10575872351855955709 matching_2 59 61 Automatic text\n",
            "10575872351855955709 matching_2 80 82 Abstract text\n",
            "10575872351855955709 matching_2 94 96 extractive text\n",
            "10575872351855955709 matching_2 106 108 biomedical text\n",
            "10575872351855955709 matching_2 145 147 automatic text\n",
            "10575872351855955709 matching_2 167 169 Persian text\n",
            "10575872351855955709 matching_2 189 191 extractive text\n",
            "10575872351855955709 matching_2 193 195 Responsive text\n",
            "12044629620993336498 matching_3 201 203 -document\n",
            "10575872351855955709 matching_2 202 204 document text\n",
            "10575872351855955709 matching_2 209 211 Extractive text\n",
            "10575872351855955709 matching_2 218 220 full text\n",
            "10575872351855955709 matching_2 231 233 redundant text\n",
            "12044629620993336498 matching_3 248 250 -document\n",
            "10575872351855955709 matching_2 307 309 automatic text\n",
            "10575872351855955709 matching_2 332 334 Abstractive text\n",
            "10575872351855955709 matching_2 352 354 automatic text\n",
            "12044629620993336498 matching_3 366 368 -document\n",
            "10575872351855955709 matching_2 367 369 document text\n",
            "10575872351855955709 matching_2 387 389 automatic text\n",
            "10575872351855955709 matching_2 405 407 Automatic Text\n",
            "14438950031246165532 matching_4 464 466 comparative study\n",
            "10575872351855955709 matching_2 482 484 automatic text\n",
            "10575872351855955709 matching_2 491 493 automatic text\n",
            "10575872351855955709 matching_2 508 510 biomedical text\n",
            "12044629620993336498 matching_3 537 539 -document\n",
            "10575872351855955709 matching_2 538 540 document text\n",
            "10575872351855955709 matching_2 554 556 extractive text\n",
            "12044629620993336498 matching_3 571 573 -document\n",
            "10575872351855955709 matching_2 572 574 document text\n",
            "10575872351855955709 matching_2 597 599 abstractive text\n",
            "10575872351855955709 matching_2 607 609 Chinese text\n",
            "10575872351855955709 matching_2 661 663 arabic text\n",
            "14438950031246165532 matching_4 772 774 multilingual study\n",
            "10575872351855955709 matching_2 778 780 language text\n",
            "12044629620993336498 matching_3 802 804 -document\n",
            "10575872351855955709 matching_2 804 806 Arabic text\n",
            "12044629620993336498 matching_3 828 830 -document\n",
            "10575872351855955709 matching_2 833 835 arabic text\n",
            "10575872351855955709 matching_2 847 849 biomedical text\n",
            "10575872351855955709 matching_2 851 853 Hybrid text\n",
            "14438950031246165532 matching_4 878 880 comparative study\n",
            "12044629620993336498 matching_3 892 894 -document\n",
            "10575872351855955709 matching_2 893 895 document text\n",
            "10575872351855955709 matching_2 911 913 extractive text\n",
            "10575872351855955709 matching_2 919 921 automatic text\n",
            "12044629620993336498 matching_3 949 951 -document\n",
            "10575872351855955709 matching_2 950 952 document text\n",
            "10575872351855955709 matching_2 985 987 arabic text\n",
            "10575872351855955709 matching_2 995 997 Abstractive text\n",
            "14438950031246165532 matching_4 1005 1007 Comparative study\n",
            "10575872351855955709 matching_2 1035 1037 arabic text\n",
            "10575872351855955709 matching_2 1047 1049 biomedical text\n",
            "10575872351855955709 matching_2 1051 1053 Extractive Text\n",
            "10575872351855955709 matching_2 1130 1132 automatic text\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9E_K5d20wPr"
      },
      "source": [
        "Applications"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDu-RTvbzz-q",
        "outputId": "c3afd1c8-a2d8-4d5a-a178-577add3c5bc3"
      },
      "source": [
        "pattern1 = [{'LOWER': 'for'},\n",
        "            {'POS':'ADJ'},{'POS':'NOUN'}]\n",
        "\n",
        "pattern2 = [{'LOWER': 'in'},{'POS':'ADJ'}]\n",
        "\n",
        "\n",
        "\n",
        "matcher = Matcher(nlp.vocab) \n",
        "matcher.add(\"matching_1\",[pattern1]) \n",
        "matcher.add(\"matching_2\",[pattern2])\n",
        "matches = matcher(doc) \n",
        "for match_id, start, end in matches:\n",
        "    string_id = nlp.vocab.strings[match_id]\n",
        "    span = doc[start:end]  \n",
        "    print(match_id, string_id, start, end, span.text)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11840699188806025751 matching_1 93 96 for extractive text\n",
            "10575872351855955709 matching_2 105 107 in biomedical\n",
            "11840699188806025751 matching_1 144 147 for automatic text\n",
            "11840699188806025751 matching_1 166 169 for Persian text\n",
            "11840699188806025751 matching_1 173 176 for big data\n",
            "10575872351855955709 matching_2 220 222 in systematic\n",
            "10575872351855955709 matching_2 266 268 in middle\n",
            "11840699188806025751 matching_1 306 309 for automatic text\n",
            "11840699188806025751 matching_1 351 354 for automatic text\n",
            "10575872351855955709 matching_2 485 487 in mobile\n",
            "10575872351855955709 matching_2 507 509 in biomedical\n",
            "11840699188806025751 matching_1 580 583 for subjective evaluation\n",
            "11840699188806025751 matching_1 728 731 for legal texts\n",
            "11840699188806025751 matching_1 984 987 for arabic text\n",
            "10575872351855955709 matching_2 1045 1047 in probabilistic\n",
            "11840699188806025751 matching_1 1129 1132 for automatic text\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dq_7VGmrsum4"
      },
      "source": [
        "## **2. Domain-specific information extraction (10 points)**\n",
        "\n",
        "For the legal case used in the data cleaning exercise: [01-05-1 Adams v Tanner.txt](https://raw.githubusercontent.com/unt-iialab/info5731_spring2021/main/class_exercises/01-05-1%20%20Adams%20v%20Tanner.txt), use [legalNLP](https://lexpredict-lexnlp.readthedocs.io/en/latest/modules/extract/extract.html#nlp-based-extraction-methods) to extract the following inforation from the text (if the information is not exist, just print None):\n",
        "\n",
        "(1) acts, e.g., “section 1 of the Advancing Hope Act, 1986”\n",
        "\n",
        "(2) amounts, e.g., “ten pounds” or “5.8 megawatts”\n",
        "\n",
        "(3) citations, e.g., “10 U.S. 100” or “1998 S. Ct. 1”\n",
        "\n",
        "(4) companies, e.g., “Lexpredict LLC”\n",
        "\n",
        "(5) conditions, e.g., “subject to …” or “unless and until …”\n",
        "\n",
        "(6) constraints, e.g., “no more than”\n",
        "\n",
        "(7) copyright, e.g., “(C) Copyright 2000 Acme”\n",
        "\n",
        "(8) courts, e.g., “Supreme Court of New York”\n",
        "\n",
        "(9) CUSIP, e.g., “392690QT3”\n",
        "\n",
        "(10) dates, e.g., “June 1, 2017” or “2018-01-01”\n",
        "\n",
        "(11) definitions, e.g., “Term shall mean …”\n",
        "\n",
        "(12) distances, e.g., “fifteen miles”\n",
        "\n",
        "(13) durations, e.g., “ten years” or “thirty days”\n",
        "\n",
        "(14) geographic and geopolitical entities, e.g., “New York” or “Norway”\n",
        "\n",
        "(15) money and currency usages, e.g., “$5” or “10 Euro”\n",
        "\n",
        "(16) percents and rates, e.g., “10%” or “50 bps”\n",
        "\n",
        "(17) PII, e.g., “212-212-2121” or “999-999-9999”\n",
        "\n",
        "(18) ratios, e.g.,” 3:1” or “four to three”\n",
        "\n",
        "(19) regulations, e.g., “32 CFR 170”\n",
        "\n",
        "(20) trademarks, e.g., “MyApp (TM)”\n",
        "\n",
        "(21) URLs, e.g., “http://acme.com/”\n",
        "\n",
        "(22) addresses, e.g., “1999 Mount Read Blvd, Rochester, NY, USA, 14615”\n",
        "\n",
        "(23) persons, e.g., “John Doe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qc7NtJrLx5tS",
        "outputId": "7c02463f-03ab-4eab-e14a-474a2431343f"
      },
      "source": [
        "!pip install lexnlp"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: lexnlp in /usr/local/lib/python3.7/dist-packages (1.8.0)\n",
            "Requirement already satisfied: pandas==0.24.2 in /usr/local/lib/python3.7/dist-packages (from lexnlp) (0.24.2)\n",
            "Requirement already satisfied: datefinder-lexpredict==0.6.2.1 in /usr/local/lib/python3.7/dist-packages (from lexnlp) (0.6.2.1)\n",
            "Requirement already satisfied: numpy==1.19.1 in /usr/local/lib/python3.7/dist-packages (from lexnlp) (1.19.1)\n",
            "Requirement already satisfied: reporters-db==2.0.3 in /usr/local/lib/python3.7/dist-packages (from lexnlp) (2.0.3)\n",
            "Requirement already satisfied: joblib==0.14.0 in /usr/local/lib/python3.7/dist-packages (from lexnlp) (0.14.0)\n",
            "Requirement already satisfied: Unidecode==1.1.1 in /usr/local/lib/python3.7/dist-packages (from lexnlp) (1.1.1)\n",
            "Requirement already satisfied: scipy==1.5.1 in /usr/local/lib/python3.7/dist-packages (from lexnlp) (1.5.1)\n",
            "Requirement already satisfied: nltk==3.5 in /usr/local/lib/python3.7/dist-packages (from lexnlp) (3.5)\n",
            "Requirement already satisfied: num2words==0.5.10 in /usr/local/lib/python3.7/dist-packages (from lexnlp) (0.5.10)\n",
            "Requirement already satisfied: pycountry==20.7.3 in /usr/local/lib/python3.7/dist-packages (from lexnlp) (20.7.3)\n",
            "Requirement already satisfied: regex==2020.7.14 in /usr/local/lib/python3.7/dist-packages (from lexnlp) (2020.7.14)\n",
            "Requirement already satisfied: scikit-learn==0.23.1 in /usr/local/lib/python3.7/dist-packages (from lexnlp) (0.23.1)\n",
            "Requirement already satisfied: dateparser==0.7.2 in /usr/local/lib/python3.7/dist-packages (from lexnlp) (0.7.2)\n",
            "Requirement already satisfied: requests==2.24.0 in /usr/local/lib/python3.7/dist-packages (from lexnlp) (2.24.0)\n",
            "Requirement already satisfied: gensim==3.8.3 in /usr/local/lib/python3.7/dist-packages (from lexnlp) (3.8.3)\n",
            "Requirement already satisfied: us==2.0.2 in /usr/local/lib/python3.7/dist-packages (from lexnlp) (2.0.2)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.7/dist-packages (from pandas==0.24.2->lexnlp) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.7/dist-packages (from pandas==0.24.2->lexnlp) (2.8.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from reporters-db==2.0.3->lexnlp) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk==3.5->lexnlp) (4.41.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk==3.5->lexnlp) (7.1.2)\n",
            "Requirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.7/dist-packages (from num2words==0.5.10->lexnlp) (0.6.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.23.1->lexnlp) (2.1.0)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.7/dist-packages (from dateparser==0.7.2->lexnlp) (1.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests==2.24.0->lexnlp) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests==2.24.0->lexnlp) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests==2.24.0->lexnlp) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests==2.24.0->lexnlp) (2.10)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.3->lexnlp) (4.2.0)\n",
            "Requirement already satisfied: jellyfish==0.6.1 in /usr/local/lib/python3.7/dist-packages (from us==2.0.2->lexnlp) (0.6.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "OaaZNeje1VIN",
        "outputId": "ce399234-fb7b-48a3-bb52-27e03e246e10"
      },
      "source": [
        "uploaded = files.upload()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9f5ee547-85c6-4af6-8d3b-0d5cc3867f75\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-9f5ee547-85c6-4af6-8d3b-0d5cc3867f75\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving 01-05-1 Adams v Tanner.txt to 01-05-1 Adams v Tanner (1).txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAhMdqhpzz-r"
      },
      "source": [
        "text = open(\"/content/01-05-1 Adams v Tanner.txt\").read()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZpdHRNL6zz-r",
        "outputId": "7cc28ac4-08c7-452e-edf0-7a77e01d8fba"
      },
      "source": [
        "import lexnlp.extract.en.acts\n",
        "print(lexnlp.extract.en.acts.get_act_list(text))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7EpKOU4gzz-s",
        "outputId": "347013ee-3735-4b3a-89fb-a339c2296de8"
      },
      "source": [
        "import lexnlp.extract.en.amounts\n",
        "list(lexnlp.extract.en.amounts.get_amounts(text))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Decimal('5.0'),\n",
              " Decimal('740.0'),\n",
              " Decimal('1843.0'),\n",
              " Decimal('2.0'),\n",
              " Decimal('1.0'),\n",
              " Decimal('4.0'),\n",
              " Decimal('2.0'),\n",
              " Decimal('1821.0'),\n",
              " Decimal('5.0'),\n",
              " Decimal('1.0'),\n",
              " Decimal('1840.0'),\n",
              " Decimal('3777.0'),\n",
              " Decimal('80.0'),\n",
              " Decimal('100.0'),\n",
              " Decimal('30.0'),\n",
              " Decimal('1839.0'),\n",
              " Decimal('741.0'),\n",
              " Decimal('22.0'),\n",
              " Decimal('1840.0'),\n",
              " Decimal('14000.0'),\n",
              " Decimal('120.0'),\n",
              " Decimal('1.0'),\n",
              " Decimal('1840.0'),\n",
              " Decimal('3.0'),\n",
              " Decimal('4.0'),\n",
              " Decimal('1.0'),\n",
              " Decimal('1.0'),\n",
              " Decimal('1840.0'),\n",
              " Decimal('2.0'),\n",
              " Decimal('1.0'),\n",
              " Decimal('361.0'),\n",
              " Decimal('1.0'),\n",
              " Decimal('307.0'),\n",
              " Decimal('6.0'),\n",
              " Decimal('604.0'),\n",
              " Decimal('1.0'),\n",
              " Decimal('2.0'),\n",
              " Decimal('418.0'),\n",
              " Decimal('422.0'),\n",
              " Decimal('7.0'),\n",
              " Decimal('34.0'),\n",
              " Decimal('41.0'),\n",
              " Decimal('167.0'),\n",
              " Decimal('742.0'),\n",
              " Decimal('3.0'),\n",
              " Decimal('112.0'),\n",
              " Decimal('207.0'),\n",
              " Decimal('3.0'),\n",
              " Decimal('338.0'),\n",
              " Decimal('424.0'),\n",
              " Decimal('5.0'),\n",
              " Decimal('26.0'),\n",
              " Decimal('13.0'),\n",
              " Decimal('235.0'),\n",
              " Decimal('8.0'),\n",
              " Decimal('693.0'),\n",
              " Decimal('4.0'),\n",
              " Decimal('1821.0'),\n",
              " Decimal('167.0'),\n",
              " Decimal('2.0'),\n",
              " Decimal('2.0'),\n",
              " Decimal('216.0'),\n",
              " Decimal('3.0'),\n",
              " Decimal('66.0'),\n",
              " Decimal('4.0'),\n",
              " Decimal('130.0'),\n",
              " Decimal('29.0'),\n",
              " Decimal('2.0'),\n",
              " Decimal('241.0'),\n",
              " Decimal('2.0'),\n",
              " Decimal('332.0'),\n",
              " Decimal('2.0'),\n",
              " Decimal('422.0'),\n",
              " Decimal('9.0'),\n",
              " Decimal('112.0'),\n",
              " Decimal('743.0'),\n",
              " Decimal('9.0'),\n",
              " Decimal('39.0'),\n",
              " Decimal('14000.0'),\n",
              " Decimal('1840.0'),\n",
              " Decimal('744.0'),\n",
              " Decimal('5.0'),\n",
              " Decimal('182.0'),\n",
              " Decimal('3.0'),\n",
              " Decimal('368.0'),\n",
              " Decimal('1.0'),\n",
              " Decimal('397.0'),\n",
              " Decimal('6.0'),\n",
              " Decimal('604.0'),\n",
              " Decimal('1.0'),\n",
              " Decimal('1821.0'),\n",
              " Decimal('167.0'),\n",
              " Decimal('745.0'),\n",
              " Decimal('4.0'),\n",
              " Decimal('746.0'),\n",
              " Decimal('4.0'),\n",
              " Decimal('210.0'),\n",
              " Decimal('46.0'),\n",
              " Decimal('747.0'),\n",
              " Decimal('5.0'),\n",
              " Decimal('5.0'),\n",
              " Decimal('740.0'),\n",
              " Decimal('1843.0'),\n",
              " Decimal('284.0'),\n",
              " Decimal('2019.0'),\n",
              " Decimal('9.0'),\n",
              " Decimal('1.0'),\n",
              " Decimal('55.0'),\n",
              " Decimal('266.0'),\n",
              " Decimal('271.0'),\n",
              " Decimal('1876.0'),\n",
              " Decimal('2.0'),\n",
              " Decimal('47.0'),\n",
              " Decimal('362.0'),\n",
              " Decimal('376.0'),\n",
              " Decimal('1872.0'),\n",
              " Decimal('3.0'),\n",
              " Decimal('45.0'),\n",
              " Decimal('329.0'),\n",
              " Decimal('334.0'),\n",
              " Decimal('1871.0'),\n",
              " Decimal('4.0'),\n",
              " Decimal('31.0'),\n",
              " Decimal('526.0'),\n",
              " Decimal('527.0'),\n",
              " Decimal('1858.0'),\n",
              " Decimal('5.0'),\n",
              " Decimal('21.0'),\n",
              " Decimal('333.0'),\n",
              " Decimal('335.0'),\n",
              " Decimal('1852.0'),\n",
              " Decimal('6.0'),\n",
              " Decimal('8.0'),\n",
              " Decimal('145.0'),\n",
              " Decimal('147.0'),\n",
              " Decimal('1857.0'),\n",
              " Decimal('7.0'),\n",
              " Decimal('65.0'),\n",
              " Decimal('256.0'),\n",
              " Decimal('258.0'),\n",
              " Decimal('3.0'),\n",
              " Decimal('1880.0'),\n",
              " Decimal('8.0'),\n",
              " Decimal('4.0'),\n",
              " Decimal('913.0'),\n",
              " Decimal('914.0'),\n",
              " Decimal('1887.0'),\n",
              " Decimal('9.0'),\n",
              " Decimal('103.0'),\n",
              " Decimal('464.0'),\n",
              " Decimal('1936.0'),\n",
              " Decimal('3.0'),\n",
              " Decimal('1.0'),\n",
              " Decimal('9.0'),\n",
              " Decimal('39.0'),\n",
              " Decimal('1828.0'),\n",
              " Decimal('2.0'),\n",
              " Decimal('2.0'),\n",
              " Decimal('5.0'),\n",
              " Decimal('182.0'),\n",
              " Decimal('1837.0'),\n",
              " Decimal('2.0'),\n",
              " Decimal('3.0'),\n",
              " Decimal('9.0'),\n",
              " Decimal('108.0'),\n",
              " Decimal('1812.0'),\n",
              " Decimal('6.0'),\n",
              " Decimal('1.0'),\n",
              " Decimal('2.0')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7XC2kB7zz-s",
        "outputId": "f3fa127d-8ce5-45e5-d7d2-dca7b1c8db3e"
      },
      "source": [
        "import lexnlp.extract.en.citations\n",
        "print(list(lexnlp.extract.en.citations.get_citations(text)))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(5, 'Ala.', 'Alabama Reports', 740, None, None, None), (5, 'Ala.', 'Alabama Reports', 740, '1843', None, None), (55, 'Ala.', 'Alabama Reports', 266, '271', None, None), (47, 'Ala.', 'Alabama Reports', 362, '376', None, None), (45, 'Ala.', 'Alabama Reports', 329, '334', None, None), (31, 'Ala.', 'Alabama Reports', 526, '527', None, None), (21, 'Ala.', 'Alabama Reports', 333, '335', None, None), (8, 'Cal.', 'California Reports', 145, '147', None, None), (65, 'Ala.', 'Alabama Reports', 256, '258', None, None), (4, 'S.W.', 'South Western Reporter', 913, '914', None, None), (103, 'A.L.R.', 'American Law Reports', 464, None, None, None), (9, 'Cow.', \"Cowen's Reports\", 39, None, None, None), (5, 'Port.', 'Alabama Reports, Porter', 182, None, None, None), (9, 'Johns.', \"Johnson's Reports\", 108, None, None, None)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ui3vxnQDzz-s",
        "outputId": "a6a60c41-d2f4-46be-b6f7-ff0cfd4aafce"
      },
      "source": [
        "import lexnlp.extract.en.entities.nltk_re\n",
        "print(list(lexnlp.extract.en.entities.nltk_re.get_companies(text)))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Lehman, Durr Co, (17983, 18001)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJCCf1HXzz-s",
        "outputId": "6438b6ad-a9bf-4c72-e58a-6d4d630bae17"
      },
      "source": [
        "import lexnlp.extract.en.conditions\n",
        "print(list(lexnlp.extract.en.conditions.get_conditions(text)))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('until', '[2]\\nCreditors’ Remedies\\nLien and Priority\\nUnder St.1821, prohibiting a levy on a crop', ''), ('until', 'on a growing crop, nor does such lien attach', ''), ('if', 'It was proved by the claimants, by the production of a written contract, that Harrison, on the twenty-second of May, 1840, in consideration that the claimants were involved, as indorsers for Burton & Harrison of Sumter county, and were then exposed to an execution, amounting to upwards of fourteen thousand dollars, bargained and sold to the claimants all his growing crop of cotton &c., consisting of one hundred and twenty acres, &c. Allen Harrison promised and obliged himself to give up his crop to the use of the claimants at any time to save them from suffering as his indorsers;', ''), ('when', 'The claimants came from Tennessee, (where they resided) about the first of September, 1840, bringing with them three or four white laborers, and took possession of the crop and slaves, and with the latter, and white laborers, gathered the cotton, prepared it for market, and', ''), ('if', 'The court charged the jury, that the plaintiff had no lien by virtue of his judgment, and execution on the growing crop; that Harrison had a right to convey it, without being in any manner restrained by them; that the writing adduced, was a sale of the crop, but', ''), ('when', 'it was not, and the lien of the fieri facias would have attached upon it,', ''), ('if', 'gathered, yet', ''), ('not subject to', 'the claimants obtained possession on the first of September, and controlled the gathering of the crop, then no lien attached, and it was', ''), ('until', 'Rep, 693;] and', ''), ('until', '167,] which declares it to be lawful to levy an execution on a planted crop,', ''), ('if', 'It is admitted that the contract between the defendant in execution, and the claimants, was in good faith,', ''), ('when', 'The defendant in execution might at any time have divested the interest which the contract vested in the claimants, by discharging their liability as his indorsers, or a judgment creditor might have satisfied the lien, and', ''), ('unless', 'We will then consider the writing under which the claimants assert a right, as a mortgage with a power to take possession any time during the year,', ''), ('if', 'Conceding the truth of the facts stated in the bill of exceptions, and we think it will not follow, that the possession of the claimants is a nullity, and that the case must be considered as', ''), ('if', 'The contract contains an express undertaking to give up the crop at any time the claimants might require it for their indemnity, and', ''), ('if', 'they took possession of it in the absence of the grantor, (though without his consent,)', ''), ('if', 'he subsequently acquiesced in it, the inference would be,', ''), ('subject to', 'Mr. Dane, in remarking upon this point, says, “The American editor of Bacon’s Abridgment, says, ‘Wheat growing in the ground is a chattel, and', ''), ('until', 'The first section of the act of 1821, “To prevent sheriffs and other officers from levying executions in certain cases, enacts, that “It shall not be lawful for any sheriff or other officer, to levy a writ of fieri facias or other execution on the planted crop of a debtor, or person against whom an execution may issue,', ''), ('until', 'Now here is an express inhibition to levy an execution on a crop while it remains on, or in the ground, and', ''), ('until', 'If so, the act cited, will only have the effect of keeping the right to levy it in abeyance', ''), ('if', 'The lien and the right to levy are intimately connected, and', ''), ('until', 'That it was competent for the legislature to have made it unlawful to levy an execution on particular property,', ''), ('until', 'If the object was merely to suspend the sale,', ''), ('as soon as', 'The idea that the lien attached upon the planted crop', ''), ('until', 'the execution was delivered to the sheriff, though the right to levy it was postponed', ''), ('if', 'They do not refer to the lien,', ''), ('until', 'they did they would postpone it', ''), ('until', 'the crop was gathered; but it is the levy they relate to and postpone', ''), ('until', '**4 The right to levy an execution on a planted crop, then, being expressly taken away by the statute, the lien which is connected with and consequent upon that right, never attaches', ''), ('if', 'The circuit judge may have mistaken the law in supposing that the contract was a sale, but', ''), ('when', 'There is no assumption of any material fact in the charge; but the possession of the claimant, the time', ''), ('if', 'acquired, the gathering of the crop, &c., are all referred to the determination of the jury; who are instructed,', ''), ('until', '**4 The statute which presents the question before the court is, that “it shall not be lawful for any sheriff or other officer to levy a writ of fieei facias or other execution, on the planted crop of a debtor, or person against whom an execution may issue,', ''), ('subject to', 'The policy of the State, as indicated by these statutes, is undeniably that all the property of a debtor, real and personal, to which he has a legal title, shall be', ''), ('until', 'The mischief which the statute designed to remedy was, the sacrifice which would be necessarily made by the sale of an immature crop: the statute enables the debtor to retain it', ''), ('if', '**5', ''), ('until', 'The sheriff is forbidden to levy on a “planted crop”', ''), ('if', 'Now,', ''), ('until', 'This, I feel a thorough conviction, was not the intention of the legislature; but that it was to secure him from loss, by prohibiting a levy and sale of the crop,', ''), ('when', 'it was gathered,', ''), ('subject to', 'Growing crops as', ''), ('subject to', '464\\nGenerally, at common law, growing crops raised by annual planting, while still attached to the soil, are regarded as personal chattels,', ''), ('where', 'And', '')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZgMLh05zz-t",
        "outputId": "a7eb7656-ece4-4f9c-a951-be89fedae4c4"
      },
      "source": [
        "import lexnlp.extract.en.constraints\n",
        "print(list(lexnlp.extract.en.constraints.get_constraints(text)))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('after', 'on a growing crop, nor does such lien attach until', ''), ('after', '', ' and that alias and pluries fieri facias’, issued regularly up to the time levy was made; that the cotton levied on was growed on the plantation of harrison, and cultivated by the hands in his service.'), ('first of', 'the claimants came from tennessee, (where they resided) about the', ''), ('first of', 'the court charged the jury, that the plaintiff had no lien by virtue of his judgment, and execution on the growing crop; that harrison had a right to convey it, without being in any manner restrained by them; that the writing adduced, was a sale of the crop, but if it was not, and the lien of the fieri facias would have attached upon it, when gathered, yet if the claimants obtained possession on the', ''), ('after', 'it merely inhibits the levy, but the lien attaches, and a levy and sale may be made', ''), ('more than', 'taking this to be clear *744 law, and it will be seen, that the defendant in execution at the time of the levy had nothing', ''), ('before', 'it has been frequently mooted whether, at common law, corn, &c.,', ''), ('before', '**4 the statute which presents the question', ''), ('after', 'now, if the view taken by the majority of the court, is correct, the right secured to the plaintiff in execution, of levying on the crop', ''), ('before', 'tried', ''), ('before', 'tried', ''), ('before', 'tried', ''), ('before', 'tried', ''), ('before', 'tried', ''), ('before', 'tried', '')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QNa9k2f-zz-t",
        "outputId": "e386583e-4b04-4406-b238-bfbe93e45a78"
      },
      "source": [
        "import lexnlp.extract.en.copyright\n",
        "print(list(lexnlp.extract.en.copyright.get_copyright(text)))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('©', '2019', 'Thomson Reuters. No')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIRN76UPzz-t",
        "outputId": "98aa56ee-ae3f-4dfd-fe9a-8bf438a55328"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "import lexnlp.extract.en.dict_entities as de\n",
        "import lexnlp.extract.en.courts\n",
        "court_config_data=[]\n",
        "court_df = pd.read_csv(\"https://raw.githubusercontent.com/LexPredict/lexpredict-legal-dictionary/1.0.5/en/legal/us_courts.csv\")\n",
        "for _, row in court_df.iterrows():\n",
        "   c = de.DictionaryEntry(row[\"Court ID\"], row[\"Court Name\"], 0, row[\"Alias\"].split(\";\") if not pd.isnull(row[\"Alias\"]) else [])\n",
        "   court_config_data.append(c)\n",
        "\n",
        "if(len(list(lexnlp.extract.en.courts.get_courts(text,court_config_data)))==0):\n",
        "  print('NONE')\n",
        "else:\n",
        "  print(list(lexnlp.extract.en.courts.get_courts(text,court_config_data)))\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NONE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Ewi42kTzz-t",
        "outputId": "3449f6f4-7f1b-45dd-aeb0-bfd02c6e5774"
      },
      "source": [
        "import lexnlp.extract.en.cusip\n",
        "\n",
        "if(len(list(lexnlp.extract.en.cusip.get_cusip(text)))==0):\n",
        "  print('NONE')\n",
        "else:\n",
        "  print(list(lexnlp.extract.en.cusip.get_cusip(text)))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NONE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwK4tN_Xzz-u",
        "outputId": "a6dc07c9-6f0e-4dd1-c59d-5c1422553164"
      },
      "source": [
        "import lexnlp.extract.en.dates\n",
        "print(list(lexnlp.extract.en.dates.get_dates(text)))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[datetime.date(2021, 6, 1), datetime.date(1840, 11, 1), datetime.date(1839, 10, 1), datetime.date(1840, 9, 1), datetime.date(1840, 5, 1), datetime.date(1840, 5, 1), datetime.date(2021, 12, 1), datetime.date(2021, 12, 1), datetime.date(2021, 1, 1), datetime.date(2021, 1, 1), datetime.date(2021, 1, 1), datetime.date(2021, 3, 21), datetime.date(2021, 6, 1), datetime.date(2021, 7, 1), datetime.date(2021, 11, 1)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvKj4pCFzz-u",
        "outputId": "b156c967-bec2-4f3f-f032-9aeec7a8d285"
      },
      "source": [
        "import lexnlp.extract.en.definitions\n",
        "if(len(list(lexnlp.extract.en.definitions.get_definitions(text)))==0):\n",
        "  print('NONE')\n",
        "else:\n",
        "  print(list(lexnlp.extract.en.definitions.get_definitions(text)))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NONE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdj8WvOVzz-u",
        "outputId": "80860352-27b6-411e-cb15-2584df641fb0"
      },
      "source": [
        "import lexnlp.extract.en.distances\n",
        "\n",
        "\n",
        "if(len(list(lexnlp.extract.en.distances.get_distances(text)))==0):\n",
        "  print('NONE')\n",
        "else:\n",
        "  print(list(lexnlp.extract.en.distances.get_distances(text)))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NONE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nStsK85xzz-u",
        "outputId": "df2fd6e7-200d-444f-8b7b-b039ffe15a1d"
      },
      "source": [
        "import lexnlp.extract.en.durations\n",
        "\n",
        "\n",
        "if(len(list(lexnlp.extract.en.durations.get_durations(text)))==0):\n",
        "  print('NONE')\n",
        "else:\n",
        "  print(list(lexnlp.extract.en.durations.get_durations(text)))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('second', Decimal('20.0'), Decimal('0.0002')), ('year', Decimal('6.0'), Decimal('2190.0'))]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pU18zXAdzz-v",
        "outputId": "19d137b4-809b-4df9-d82f-59e8cbbd00af"
      },
      "source": [
        "import lexnlp.extract.en.geoentities\n",
        "geo_config_list = []\n",
        "list(lexnlp.extract.en.geoentities.get_geoentities(text,geo_config_list))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOOg62WFzz-v",
        "outputId": "763f5ace-323a-4208-d2e1-02b0def263cf"
      },
      "source": [
        "import lexnlp.extract.en.money\n",
        "\n",
        "\n",
        "if(len(list(lexnlp.extract.en.money.get_money(text)))==0):\n",
        "  print('NONE')\n",
        "else:\n",
        "  print(list(lexnlp.extract.en.money.get_money(text)))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(Decimal('100.0'), 'USD'), (Decimal('14000.0'), 'USD'), (Decimal('14000.0'), 'USD')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5h-JbE3zz-v",
        "outputId": "e94ce70e-5b31-4a9f-a077-9b7039728209"
      },
      "source": [
        "import lexnlp.extract.en.percents\n",
        "\n",
        "\n",
        "if(len(list(lexnlp.extract.en.percents.get_percents(text)))==0):\n",
        "  print('NONE')\n",
        "else:\n",
        "  print(list(lexnlp.extract.en.percents.get_percents(text)))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NONE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncCVadWYzz-v",
        "outputId": "4afba36a-7b13-49f3-8f31-ea7566908eef"
      },
      "source": [
        "import lexnlp.extract.en.pii\n",
        "\n",
        "if(len(list(lexnlp.extract.en.pii.get_pii(text)))==0):\n",
        "  print('NONE')\n",
        "else:\n",
        "  print(list(lexnlp.extract.en.pii.get_pii(text)))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NONE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdtj0ksXzz-v",
        "outputId": "077082ce-09ce-4079-d0f2-146f3fecd42d"
      },
      "source": [
        "import lexnlp.extract.en.ratios\n",
        "\n",
        "\n",
        "if(len(list(lexnlp.extract.en.ratios.get_ratios(text)))==0):\n",
        "  print('NONE')\n",
        "else:\n",
        "  print(list(lexnlp.extract.en.ratios.get_ratios(text)))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NONE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucgeDXMYzz-w",
        "outputId": "30003566-57ec-4712-e896-de83ae924b0e"
      },
      "source": [
        "import lexnlp.extract.en.regulations\n",
        "\n",
        "\n",
        "if(len(list(lexnlp.extract.en.regulations.get_regulations(text)))==0):\n",
        "  print('NONE')\n",
        "else:\n",
        "  print(list(lexnlp.extract.en.regulations.get_regulations(text)))\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NONE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwKz90s8zz-w",
        "outputId": "fbc83524-1442-49c6-9a98-3a34cc3af592"
      },
      "source": [
        "import lexnlp.extract.en.trademarks\n",
        "\n",
        "\n",
        "if(len(list(lexnlp.extract.en.trademarks.get_trademarks(text)))==0):\n",
        "  print('NONE')\n",
        "else:\n",
        "  print(list(lexnlp.extract.en.trademarks.get_trademarks(text)))\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NONE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAB9pCZTzz-w",
        "outputId": "31c018a5-a751-4bb2-9eaf-6ece301ea4c4"
      },
      "source": [
        "import lexnlp.extract.en.urls\n",
        "\n",
        "\n",
        "if(len(list(lexnlp.extract.en.urls.get_urls(text)))==0):\n",
        "  print('NONE')\n",
        "else:\n",
        "  print(list(lexnlp.extract.en.urls.get_urls(text)))\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NONE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1ad3Bvk3e6M"
      },
      "source": [
        "import lexnlp.extract.en.entities.stanford_ner\r\n",
        "print(list(lexnlp.extract.en.entities.stanford_ner.get_persons(text)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhrAKWkBPX0m"
      },
      "source": [
        "import lexnlp.extract.en.entities.stanford_ner\r\n",
        "print(list(lexnlp.extract.en.entities.stanford_ner.get_locations(text)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjviaXjgPvOL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}